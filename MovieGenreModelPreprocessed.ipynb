{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1d294242",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "import random\n",
    "\n",
    "short_form_dic = {\"isn't\":\"is not\", \"isnt\":\"is not\",\n",
    "                 \"aren't\":\"are not\", \"arent\":\"are not\",\n",
    "                 \"wasn't\":\"was not\", \"wasnt\":\"was not\",\n",
    "                 \"weren't\":\"were not\", \"werent\":\"were not\",\n",
    "                 \"haven't\":\"have not\", \"havent\":\"have not\",\n",
    "                 \"hasn't\":\"has not\", \"hasnt\":\"has not\",\n",
    "                 \"hadn't\":\"had not\", \"hadnt\":\"had not\",\n",
    "                 \"won't\":\"will not\", \"wont\":\"will not\",\n",
    "                 \"wouldn't\":\"would not\", \"wouldnt\":\"would not\", \"wudnt\": \"would not\",\n",
    "                 \"don't\":\"do not\", \"dont\":\"do not\",\"don’t\":\"do not\",\n",
    "                 \"doesn't\":\"does not\", \"doesnt\":\"does not\",\n",
    "                 \"didn't\":\"did not\", \"didnt\":\"did not\",\n",
    "                 \"can't\":\"can not\", \"cant\":\"can not\",\n",
    "                 \"couldn't\":\"could not\", \"couldnt\":\"could not\",\n",
    "                 \"shouldn't\":\"should not\", \"shouldnt\":\"should not\", \"shudnt\":\"should not\",\n",
    "                 \"mightn't\":\"might not\", \"mightnt\":\"might not\",\n",
    "                 \"mustn't\":\"must not\", \"mustnt\":\"must not\",\n",
    "                 \"tho\":\"though\", \"w/\":\"with\", \"w/out\":\"without\",\n",
    "                 \"gonna\":\"going to\", \"i'll\":\"i will\", \"i'm\":\"i am\",\n",
    "                 \"needn’t\":\"need not\", \"neednt\":\"need not\",\n",
    "                 \"daren’t\":\"dare not\", \"darent\":\"dare not\",\n",
    "                 \"he's\":\"he is\",\"she's\":\"she is\", \"i've\": \"i have\",\n",
    "                 \"he'll\":\"he will\", \"she'll\":\"she will\", \"i'll\":\"i will\",\n",
    "                 \"we're\":\"we are\", \"skek\":\"scared\",\"ynwa\":\"you never walk alone\",\n",
    "                 \"shoulda\":\"should have\",\"shite\":\"shit\",\"snek\":\"snake\",\n",
    "                 \"can't've\": \"cannot have\", \"cant've\": \"cannot have\",\"cantve\": \"cannot have\",\n",
    "                 \"'cause\": \"because\", \"could've\": \"could have\", \"couldve\": \"could have\",\n",
    "                 \"couldn't've\": \"could not have\", \"couldnt've\": \"could not have\", \"couldntve\": \"could not have\",\n",
    "                 \"hadn't've\": \"had not have\", \"hadnt've\": \"had not have\", \"hadntve\": \"had not have\",\n",
    "                 \"he'd\": \"he would\", \"hed\": \"he would\", \n",
    "                 \"he'd've\": \"he would have\", \"hed've\": \"he would have\",\"hedve\": \"he would have\",\n",
    "                 \"he'll've\": \"he will have\", \"hell've\": \"he will have\", \"hellve\": \"he will have\",\n",
    "                 \"how'd\": \"how did\", \"how'd'y\": \"how do you\", \n",
    "                 \"how'll\": \"how will\", \"how's\": \"how is\",  \n",
    "                 \"i'd\": \"i would\", \"i'd've\": \"i would have\",\n",
    "                 \"i'll've\": \"i will have\",\n",
    "                 \"it'd\": \"it would\", \"itd\": \"it would\",\n",
    "                 \"it'd've\": \"it would have\", \"itd've\": \"it would have\",\"itdve\": \"it would have\",\n",
    "                 \"it'll\": \"it will\", \"itll\": \"it will\", \n",
    "                 \"it'll've\": \"it will have\",\"itll've\": \"it will have\",\"itllve\": \"it will have\",\n",
    "                 \"it's\": \"it is\", \"its\": \"it is\",\n",
    "                 \"let's\": \"let us\", \"lets\": \"let us\",\"ma'am\": \"madam\", \"maam\": \"madam\",\n",
    "                 \"mayn't\": \"may not\", \"maynt\": \"may not\",\n",
    "                 \"might've\": \"might have\",\"mightve\": \"might have\",\n",
    "                 \"mightn't\": \"might not\", \"mightnt\": \"might not\",\n",
    "                 \"mightn't've\": \"might not have\", \"mightnt've\": \"might not have\", \"mightntve\": \"might not have\", \n",
    "                 \"must've\": \"must have\",\"mustve\": \"must have\", \"mustn't\": \"must not\", \"mustnt\": \"must not\",\n",
    "                 \"mustn't've\": \"must not have\", \"mustnt've\": \"must not have\",\"mustntve\": \"must not have\",\n",
    "                 \"needn't\": \"need not\", \"neednt\": \"need not\",\n",
    "                 \"needn't've\": \"need not have\",\"neednt've\": \"need not have\",\"needntve\": \"need not have\",\n",
    "                 \"o'clock\": \"of the clock\", \"oclock\": \"of the clock\",\n",
    "                 \"oughtn't\": \"ought not\", \"oughtnt\": \"ought not\",\n",
    "                 \"oughtn't've\": \"ought not have\", \"oughtnt've\": \"ought not have\",\"oughtntve\": \"ought not have\",\n",
    "                 \"shan't\": \"shall not\",\"shant\": \"shall not\",\n",
    "                 \"sha'n't\": \"shall not\", \"shan't\": \"shall not\", \"shant\": \"shall not\", \n",
    "                 \"shan't've\": \"shall not have\",\"shant've\": \"shall not have\",\"shantve\": \"shall not have\",\n",
    "                 \"she'd\": \"she would\",\"shed\": \"she would\", \n",
    "                 \"she'd've\": \"she would have\", \"shed've\": \"she would have\",\"shedve\": \"she would have\",\n",
    "                 \"she'll've\": \"she will have\", \"shell've\": \"she will have\", \"shellve\": \"she will have\", \n",
    "                 \"should've\": \"should have\",\"shouldve\": \"should have\",\n",
    "                 \"shouldn't\": \"should not\",\"shouldnt\": \"should not\",\n",
    "                 \"shouldn't've\": \"should not have\", \"shouldnt've\": \"should not have\",\"shouldntve\": \"should not have\",\n",
    "                 \"so've\": \"so have\",\"so's\": \"so as\", \n",
    "                 \"this's\": \"this is\",\n",
    "                 \"that'd\": \"that would\", \"thatd\": \"that would\",\n",
    "                 \"that'd've\": \"that would have\",\"thatd've\": \"that would have\",\"thatdve\": \"that would have\",\n",
    "                 \"that's\": \"that is\", \"thats\": \"that is\",\n",
    "                 \"there'd\": \"there would\", \"thered\": \"there would\",\n",
    "                 \"there'd've\": \"there would have\",\"thered've\": \"there would have\",\"theredve\": \"there would have\",\n",
    "                 \"there's\": \"there is\",\"theres\": \"there is\",\"here's\": \"here is\",\"heres\": \"here is\",\n",
    "                 \"they'd\": \"they would\", \"theyd\": \"they would\",\n",
    "                 \"they'd've\": \"they would have\", \"theyd've\": \"they would have\",\"theydve\": \"they would have\",\n",
    "                 \"they'll\": \"they will\",\n",
    "                 \"they'll've\": \"they will have\", \"theyll've\": \"they will have\",\"theyllve\": \"they will have\",\n",
    "                 \"they're\": \"they are\", \"theyre\": \"they are\",\n",
    "                 \"they've\": \"they have\", \"theyve\": \"they have\", \n",
    "                 \"to've\": \"to have\", \"we'd\": \"we would\", \n",
    "                 \"we'd've\": \"we would have\",\"we'll've\": \"we will have\", \n",
    "                 \"we've\": \"we have\",\n",
    "                 \"what'll\": \"what will\", \"what'll've\": \"what will have\", \"what're\": \"what are\", \n",
    "                 \"what's\": \"what is\", \"whats\": \"what is\",\n",
    "                 \"what've\": \"what have\", \"when's\": \"when is\", \n",
    "                 \"when've\": \"when have\", \"where'd\": \"where did\", \"where's\": \"where is\", \n",
    "                 \"where've\": \"where have\", \"who'll\": \"who will\", \"who'll've\": \"who will have\", \n",
    "                 \"who's\": \"who is\",\"whos\": \"who is\", \"who've\": \"who have\", \"why's\": \"why is\", \"whys\": \"why is\",\n",
    "                 \"why've\": \"why have\", \"will've\": \"will have\", \n",
    "                 \"won't\": \"will not\", \"wont\": \"will not\",\n",
    "                 \"won't've\": \"will not have\", \"would've\": \"would have\",\"wouldve\": \"would have\",\n",
    "                 \"wouldn't've\": \"would not have\", \"y'all\": \"you all\", \"yall\": \"you all\", \n",
    "                 \"y'all'd\": \"you all would\",\"yall'd\": \"you all would\",\n",
    "                 \"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\"y'all've\": \"you all have\",\n",
    "                 \"you'd\": \"you would\", \"you'd've\": \"you would have\", \"you'll\": \"you will\", \n",
    "                 \"you'll've\": \"you will have\", \"you're\": \"you are\",\"youre\": \"you are\", \"you've\": \"you have\",\n",
    "                 \"youve\": \"you have\",\n",
    "               }\n",
    "     \n",
    "short_form_pattern = re.compile(r'\\b(' + '|'.join(short_form_dic.keys()) + r')\\b')\n",
    "\n",
    "def preprocess_text(example):\n",
    "    text = str(example['text'].lower())\n",
    "    try:\n",
    "        text = short_form_pattern.sub(lambda x: short_form_dic[x.group()], text)#replace short form\n",
    "    except:\n",
    "        text = text\n",
    "    text = re.sub(\"[^0-9a-zA-Z]\", \" \", text)\n",
    "    text = re.sub(r'(.)\\1+', r'\\1\\1', text)#correcting spell\n",
    "    try:\n",
    "        text = short_form_pattern.sub(lambda x: short_form_dic[x.group()], text)#replace short form again\n",
    "    except:\n",
    "        text = text\n",
    "    text = re.sub(' +',' ', text)\n",
    "    example['text'] = \"\".join(text).strip()\n",
    "    return example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cb020c6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token is valid.\n",
      "Your token has been saved in your configured git credential helpers (manager).\n",
      "Your token has been saved to C:\\Users\\hazqe\\.cache\\huggingface\\token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "db96a3ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ed3b78a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset parquet (C:/Users/hazqe/.cache/huggingface/datasets/datadrivenscience___parquet/datadrivenscience--movie-genre-prediction-01acd85570f2b187/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.010532855987548828,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 29,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 2,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d16c86b077374472babde784f5ff849c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "movie_dataset = load_dataset(\"datadrivenscience/movie-genre-prediction\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2abec6f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    test: Dataset({\n",
       "        features: ['id', 'movie_name', 'synopsis', 'genre'],\n",
       "        num_rows: 36000\n",
       "    })\n",
       "    train: Dataset({\n",
       "        features: ['id', 'movie_name', 'synopsis', 'genre'],\n",
       "        num_rows: 54000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movie_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "26463e67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 16863,\n",
       " 'movie_name': 'A Death Sentence',\n",
       " 'synopsis': \"12 y.o. Ida's dad'll die without a DKK1,500,000 operation. Ida plans to steal the money from the bank, her mom installed alarm systems in. She'll need her climbing skills, her 2 friends and 3 go-karts.\",\n",
       " 'genre': 'action'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movie_dataset[\"test\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f958ad72",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 44978,\n",
       " 'movie_name': 'Super Me',\n",
       " 'synopsis': 'A young scriptwriter starts bringing valuable objects back from his short nightmares of being chased by a demon. Selling them makes him rich.',\n",
       " 'genre': 'fantasy'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movie_dataset[\"train\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a0b33a0f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'action',\n",
       " 'adventure',\n",
       " 'crime',\n",
       " 'family',\n",
       " 'fantasy',\n",
       " 'horror',\n",
       " 'mystery',\n",
       " 'romance',\n",
       " 'scifi',\n",
       " 'thriller'}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(movie_dataset[\"train\"][\"genre\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e12ba559",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"prajjwal1/bert-medium\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ec205ab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "model_tokenizer = AutoTokenizer.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1360d1c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_len_movie_name=180\n",
      "max_len_synopsis=400\n"
     ]
    }
   ],
   "source": [
    "max_len_movie_name = 0\n",
    "max_len_synopsis = 0\n",
    "for i in range(len(movie_dataset[\"train\"])):\n",
    "    curr_len_movie_name = len(movie_dataset[\"train\"][i][\"movie_name\"])\n",
    "    if curr_len_movie_name > max_len_movie_name:\n",
    "        max_len_movie_name = curr_len_movie_name\n",
    "        \n",
    "    curr_len_synopsis = len( movie_dataset[\"train\"][i][\"synopsis\"])\n",
    "    if curr_len_synopsis > max_len_synopsis:\n",
    "        max_len_synopsis = curr_len_synopsis\n",
    "    \n",
    "print(f\"{max_len_movie_name=}\")\n",
    "print(f\"{max_len_synopsis=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3ac4ce55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_len_movie_name=100\n",
      "max_len_synopsis=400\n"
     ]
    }
   ],
   "source": [
    "max_len_movie_name = 0\n",
    "max_len_synopsis = 0\n",
    "for i in range(len(movie_dataset[\"test\"])):\n",
    "    curr_len_movie_name = len(movie_dataset[\"test\"][i][\"movie_name\"])\n",
    "    if curr_len_movie_name > max_len_movie_name:\n",
    "        max_len_movie_name = curr_len_movie_name\n",
    "        \n",
    "    curr_len_synopsis = len( movie_dataset[\"test\"][i][\"synopsis\"])\n",
    "    if curr_len_synopsis > max_len_synopsis:\n",
    "        max_len_synopsis = curr_len_synopsis\n",
    "    \n",
    "print(f\"{max_len_movie_name=}\")\n",
    "print(f\"{max_len_synopsis=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9c66a107",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': [44978, 50185, 34131],\n",
       " 'movie_name': ['Super Me',\n",
       "  'Entity Project',\n",
       "  'Behavioral Family Therapy for Serious Psychiatric Disorders'],\n",
       " 'synopsis': ['A young scriptwriter starts bringing valuable objects back from his short nightmares of being chased by a demon. Selling them makes him rich.',\n",
       "  'A director and her friends renting a haunted house to capture paranormal events in order to prove it and become popular.',\n",
       "  'This is an educational video for families and family therapists that describes the Behavioral Family Therapy approach to dealing with serious psychiatric illnesses.'],\n",
       " 'genre': ['fantasy', 'horror', 'family']}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movie_dataset[\"train\"][:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f062d8c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'movie_name', 'synopsis', 'genre'],\n",
       "    num_rows: 54000\n",
       "})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movie_dataset[\"train\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "65566f2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at C:\\Users\\hazqe\\.cache\\huggingface\\datasets\\datadrivenscience___parquet\\datadrivenscience--movie-genre-prediction-01acd85570f2b187\\0.0.0\\2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec\\cache-2edecd240afbe914.arrow\n",
      "Loading cached processed dataset at C:\\Users\\hazqe\\.cache\\huggingface\\datasets\\datadrivenscience___parquet\\datadrivenscience--movie-genre-prediction-01acd85570f2b187\\0.0.0\\2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec\\cache-6c3380305fb9eda0.arrow\n",
      "Loading cached processed dataset at C:\\Users\\hazqe\\.cache\\huggingface\\datasets\\datadrivenscience___parquet\\datadrivenscience--movie-genre-prediction-01acd85570f2b187\\0.0.0\\2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec\\cache-285cce4af61b6ebd.arrow\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'genre', 'text'],\n",
       "    num_rows: 54000\n",
       "})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = movie_dataset[\"train\"]\n",
    "dataset = dataset.map(lambda example: {'text': (example['movie_name'] + ' is a movie about ' + example['synopsis']).lower()},\n",
    "                      remove_columns=['movie_name', 'synopsis'])\n",
    "dataset = dataset.class_encode_column(\"genre\")\n",
    "dataset = dataset = dataset.map(preprocess_text)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "940670d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': Value(dtype='int64', id=None),\n",
       " 'genre': ClassLabel(names=['action', 'adventure', 'crime', 'family', 'fantasy', 'horror', 'mystery', 'romance', 'scifi', 'thriller'], id=None),\n",
       " 'text': Value(dtype='string', id=None)}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5eb494bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': Value(dtype='int64', id=None),\n",
       " 'genre': ClassLabel(names=['action', 'adventure', 'crime', 'family', 'fantasy', 'horror', 'mystery', 'romance', 'scifi', 'thriller'], id=None),\n",
       " 'text': Value(dtype='string', id=None)}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "700c0547",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 44978,\n",
       " 'genre': 4,\n",
       " 'text': 'super me is a movie about a young scriptwriter starts bringing valuable objects back from his short nightmares of being chased by a demon selling them makes him rich'}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "87c9c3ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_dataset = dataset.train_test_split(test_size=0.2, shuffle=True,stratify_by_column=\"genre\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a5078fa9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'genre', 'text'],\n",
       "        num_rows: 43200\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['id', 'genre', 'text'],\n",
       "        num_rows: 10800\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3f0f6dfb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': [23018, 3994, 57400],\n",
       " 'genre': [2, 0, 6],\n",
       " 'text': ['obsession is a movie about a diabolic suspense which never gives the viewer any respite shall helene save her husband but it may mean sending an innocent man to the guillotine',\n",
       "  'blackout is a movie about a skilled assassin wakes up with amnesia and must piece together',\n",
       "  'phantom of the rue morgue is a movie about when several women are found mutilated and murdered the paris police are baffled as to who the killer may be all evidence points to dupin but soon it becomes apparent that it is someone or something stronger and deadlier than a human']}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_dataset[\"train\"][:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c38720f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': [78712, 1426, 54436],\n",
       " 'genre': [8, 0, 6],\n",
       " 'text': ['dexter s laboratory ego trip is a movie about after dexter is confronted with robots who wish to destroy the one who saved the future he uses his time machine to see how he saved it',\n",
       "  'rag taggers is a movie about rag taggers wga reg two young american guys have to man up and save a group of usa citizens on foreign soil',\n",
       "  'the bhen jah man is a movie about after the masked serial killer known as the bhen jah man begins terrorizing the city of wichita it is up to three bonzos and a resentful detective to bring him down as they unmask the truth in this mystery comedy film']}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_dataset[\"test\"][:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "987bd422",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_len_text=175\n"
     ]
    }
   ],
   "source": [
    "max_len_text = 0\n",
    "for i in range(len(new_dataset[\"train\"])):\n",
    "    curr_len_text = len(new_dataset[\"train\"][i][\"text\"])\n",
    "    if curr_len_movie_name > max_len_text:\n",
    "        max_len_text = curr_len_text\n",
    "    \n",
    "print(f\"{max_len_text=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "81cbacd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_len_text=181\n"
     ]
    }
   ],
   "source": [
    "max_len_text = 0\n",
    "for i in range(len(new_dataset[\"test\"])):\n",
    "    curr_len_text = len(new_dataset[\"test\"][i][\"text\"])\n",
    "    if curr_len_movie_name > max_len_text:\n",
    "        max_len_text = curr_len_text\n",
    "    \n",
    "print(f\"{max_len_text=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d6c9a541",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.009986162185668945,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 29,
       "postfix": null,
       "prefix": "Map",
       "rate": null,
       "total": 43200,
       "unit": " examples",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/43200 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.02548694610595703,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 29,
       "postfix": null,
       "prefix": "Map",
       "rate": null,
       "total": 10800,
       "unit": " examples",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/10800 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'id': Value(dtype='int64', id=None),\n",
       " 'labels': ClassLabel(names=['action', 'adventure', 'crime', 'family', 'fantasy', 'horror', 'mystery', 'romance', 'scifi', 'thriller'], id=None),\n",
       " 'text': Value(dtype='string', id=None),\n",
       " 'input_ids': Sequence(feature=Value(dtype='int32', id=None), length=-1, id=None),\n",
       " 'token_type_ids': Sequence(feature=Value(dtype='int8', id=None), length=-1, id=None),\n",
       " 'attention_mask': Sequence(feature=Value(dtype='int8', id=None), length=-1, id=None)}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id,max_length=512)\n",
    "\n",
    "def process(examples):\n",
    "    tokenized_inputs = tokenizer(\n",
    "        examples[\"text\"], truncation=True, max_length=512\n",
    "    )\n",
    "    return tokenized_inputs\n",
    "\n",
    "tokenized_datasets = new_dataset.map(process, batched=True)\n",
    "tokenized_datasets = tokenized_datasets.rename_column(\"genre\",\"labels\")\n",
    "\n",
    "tokenized_datasets[\"train\"].features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "021c6afe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification, DataCollatorWithPadding\n",
    "from huggingface_hub import HfFolder\n",
    "\n",
    "# create label2id, id2label dicts for nice outputs for the model\n",
    "labels = tokenized_datasets[\"train\"].features[\"labels\"].names\n",
    "num_labels = len(labels)\n",
    "label2id, id2label = dict(), dict()\n",
    "for i, label in enumerate(labels):\n",
    "    label2id[label] = str(i)\n",
    "    id2label[str(i)] = label\n",
    "\n",
    "# define data_collator\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ba5fb12c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at prajjwal1/bert-medium were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.bias', 'cls.predictions.decoder.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at prajjwal1/bert-medium and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import TrainingArguments,Trainer\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"bert-medium-movie-genre-preprocessed\",\n",
    "    num_train_epochs=9,\n",
    "    per_device_train_batch_size=128,\n",
    "    per_device_eval_batch_size=128,\n",
    "    fp16=True,\n",
    "    learning_rate=1e-5,\n",
    "    warmup_ratio = 0.1,\n",
    "    weight_decay = 0.01,\n",
    "    adam_epsilon = 1e-8,\n",
    "    seed=39,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"accuracy\",\n",
    ")\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_id,\n",
    "    num_labels=num_labels,\n",
    "    id2label=id2label,\n",
    "    label2id=label2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "84600c3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import evaluate\n",
    "\n",
    "metric = evaluate.load(\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f844c45d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return metric.compute(predictions=predictions, references=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ece9a44f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hp_space(trial):\n",
    "    return {\n",
    "        \"num_train_epochs\": trial.suggest_int(\"num_train_epochs\", 3, 9),\n",
    "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 9e-7, 1e-3 ,log=True),\n",
    "        \"weight_decay\": trial.suggest_float(\"weight_decay\", 1e-3, 0.1 ,log=True),\n",
    "        \"adam_epsilon\": trial.suggest_float(\"adam_epsilon\", 1e-13, 1e-7 ,log=True),\n",
    "      }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "283289c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at prajjwal1/bert-medium were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.bias', 'cls.predictions.decoder.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at prajjwal1/bert-medium and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "[I 2023-06-12 23:02:53,784] A new study created in memory with name: no-name-17e9dc60-3b58-4ed4-9958-fbf94a1d1868\n",
      "Some weights of the model checkpoint at prajjwal1/bert-medium were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.bias', 'cls.predictions.decoder.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at prajjwal1/bert-medium and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\hazqe\\AAAPython\\Anaconda\\envs\\ai_triage\\lib\\site-packages\\transformers\\optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='410' max='1352' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 410/1352 03:47 < 08:44, 1.80 it/s, Epoch 1.21/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.866351</td>\n",
       "      <td>0.349167</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def student_init():\n",
    "    return AutoModelForSequenceClassification.from_pretrained(\n",
    "        model_id,\n",
    "        num_labels=num_labels,\n",
    "        id2label=id2label,\n",
    "        label2id=label2id\n",
    "    )\n",
    "\n",
    "trainer = Trainer(\n",
    "    model_init=student_init,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"test\"],\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "best_run = trainer.hyperparameter_search(\n",
    "    n_trials=139,\n",
    "    direction=\"maximize\",\n",
    "    hp_space=hp_space\n",
    ")\n",
    "\n",
    "print(best_run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d2e25d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "for k,v in best_run.hyperparameters.items():\n",
    "    setattr(training_args, k, v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e474d954",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimal_trainer = Trainer(\n",
    "    model,\n",
    "    training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"validation\"],\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "optimal_trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66eadd6d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
